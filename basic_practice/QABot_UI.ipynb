{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e85bf95-3019-4ef3-8003-dfec3c870615",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import os\n",
    "import time\n",
    "\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.document_loaders import UnstructuredFileLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.chains import RetrievalQAWithSourcesChain\n",
    "from huggingface_hub import notebook_login\n",
    "from transformers import pipeline\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from langchain import HuggingFacePipeline\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "import textwrap\n",
    "import sys\n",
    "import torch\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960ceedc-5451-4966-affb-e09ad13787e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class QA_LangChain():\n",
    "    def __init__(self):\n",
    "        self.llm = None\n",
    "    \n",
    "    def QA_LangChain_RQA_chain(self, input_doc_pth):\n",
    "        # load embedding model\n",
    "        print(\"===== Load the embedding model =====\")\n",
    "        embeddings = HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2',model_kwargs={'device': 'cpu'})\n",
    "        vectorstore=FAISS.load_local(input_doc_pth, embeddings)\n",
    "        \n",
    "        # Create a QA chain\n",
    "        print(\"===== Create a QA chain =====\")    \n",
    "        chain = RetrievalQA.from_chain_type(llm=self.llm, chain_type=\"stuff\", return_source_documents=True, retriever=vectorstore.as_retriever())\n",
    "        \n",
    "        return chain\n",
    "    \n",
    "    def QA_LangChain_RQA_model_create(self, llm_pth):\n",
    "        \n",
    "        # Load LLM model\n",
    "        print(\"===== Load LLM model =====\")\n",
    "        tokenizer = AutoTokenizer.from_pretrained(llm_pth)\n",
    "        model = AutoModelForCausalLM.from_pretrained(llm_pth, device_map='auto',\n",
    "                           torch_dtype=torch.float16,\n",
    "                           use_auth_token=True,\n",
    "                           #load_in_8bit=True,\n",
    "                            #load_in_4bit=True\n",
    "                                                    )\n",
    "        pipe = pipeline(\"text-generation\",\n",
    "             model=model,\n",
    "             tokenizer= tokenizer,\n",
    "             torch_dtype=torch.bfloat16,\n",
    "             device_map=\"auto\",\n",
    "             max_new_tokens = 1024,\n",
    "             do_sample=True,\n",
    "             top_k=10,\n",
    "             num_return_sequences=1,\n",
    "             eos_token_id=tokenizer.eos_token_id\n",
    "                       )\n",
    "        \n",
    "        # Create LLM model                                              \n",
    "        self.llm=HuggingFacePipeline(pipeline=pipe, model_kwargs={'temperature':0})\n",
    "\n",
    "class QA_UI():\n",
    "    def __init__(self):\n",
    "        self.qa_doc_dir = r'D:\\nu_QA_data'\n",
    "        #self.llm_model_path = r'C:\\Users\\USER\\Desktop\\llma\\text-generation-webui\\models\\Llama-2-7b-chat-hf'\n",
    "        self.llm_model_path = r'C:\\Users\\USER\\Desktop\\llma\\text-generation-webui\\models\\stabilityai_StableBeluga-7B'\n",
    "        self.qa_chain = None\n",
    "        self.doc_path = r'D:\\nu_QA_data\\m460bsp_easy_wo_headers'\n",
    "        \n",
    "        # We need to initial the llm model when intial the class & can't create another\n",
    "        self.QA_LangChain_CLS = None\n",
    "        self.init_chat_qa_bot()\n",
    "        \n",
    "    def init_chat_qa_bot(self):\n",
    "        self.QA_LangChain_CLS = QA_LangChain()\n",
    "        self.QA_LangChain_CLS.QA_LangChain_RQA_model_create(self.llm_model_path)\n",
    "        self.qa_chain = self.QA_LangChain_CLS.QA_LangChain_RQA_chain(self.doc_path)\n",
    "\n",
    "    def QA_gradio_UI(self):\n",
    "        \n",
    "        def start_qa_chain(doc_name):\n",
    "            self.doc_path = os.path.join(self.qa_doc_dir, doc_name)\n",
    "            #print(self.doc_path)\n",
    "            #time.sleep(2)\n",
    "            self.qa_chain = self.QA_LangChain_CLS.QA_LangChain_RQA_chain(self.doc_path)\n",
    "  \n",
    "            return \"{} finish !\".format(doc_name.split('_index')[0])\n",
    "        \n",
    "        inputs = [gr.Dropdown(next(os.walk(self.qa_doc_dir))[1], label=\"Choose Documents\", info=self.qa_doc_dir, value=self.doc_path.split('\\\\')[-1]),\n",
    "            ]\n",
    "        outputs = [gr.Textbox(label='Loading document now:', value=self.doc_path.split('\\\\')[-1])\n",
    "            ]\n",
    "        \n",
    "        with gr.Blocks() as demo:\n",
    "            \n",
    "            # Layout Section\n",
    "            with gr.Tab(\"QA ChatBot\"):\n",
    "                gr.Markdown(\n",
    "                    value= \"LLM model: {}\".format(self.llm_model_path.split(\"\\\\\")[-1])\n",
    "                )\n",
    "                with gr.Row():\n",
    "                    def update_dropdown_list():\n",
    "                        return [gr.Dropdown(choices=next(os.walk(self.qa_doc_dir))[1]), 'loading...']\n",
    "                    \n",
    "                    gr.Interface(fn=start_qa_chain, inputs=inputs, outputs=outputs)\n",
    "                    # if dropdown is been selected, it will refresh 1 time\n",
    "                    inputs[0].select(fn=update_dropdown_list, inputs=[], outputs=[inputs[0], outputs[0]])\n",
    "                    \n",
    "                chatbot = gr.Chatbot(height = 600, show_copy_button = True, scale = 2)\n",
    "                msg = gr.Textbox()\n",
    "                with gr.Row():\n",
    "                    enter_button = gr.Button(\"Enter\")\n",
    "                    save_button = gr.Button(\"Save\")\n",
    "                    clear = gr.Button(\"Clear\")\n",
    "                    #delete_button = gr.Button(\"delete model\")\n",
    "            \n",
    "            with gr.Tab(\"Question Example\"):\n",
    "                gr.Markdown(\n",
    "                        \"\"\"\n",
    "                        # Nuvoton-MCU-PSG\n",
    "                        #### What are the key features of M460 Series?\n",
    "                        #### What is the operating voltage of M031G?\n",
    "                        #### What is the architecture of NuMicro M23?\n",
    "                        \"\"\"\n",
    "                )\n",
    "            \n",
    "            with gr.Tab(\"Documents (ToDo...)\"):\n",
    "                file_loader = gr.File(file_count=\"multiple\", file_types=[\".txt\", \".pdf\", \".csv\"], height=100)\n",
    "                state_text = gr.Textbox(label='model state:', value=\"Please load a document.\")\n",
    "            \n",
    "            \n",
    "            # Events Section\n",
    "            def load_state_log():\n",
    "                return \"Load the document and model successfully\"\n",
    "            \n",
    "            def upload_file(files):\n",
    "                file_paths = [file.name for file in files]\n",
    "                return file_paths\n",
    "            \n",
    "            def update_doc_and_reload_QA_sys(files):\n",
    "                file_paths = [file.name for file in files]\n",
    "                \n",
    "            def user(user_message, history):\n",
    "                return \"\", history + [[user_message, None]]\n",
    "        \n",
    "            def bot(history):\n",
    "                # Get the user's Q msg\n",
    "                # print(history[-1][0]) \n",
    "                # query = \"What is the architecture of NuMicro M23\"\n",
    "                query = history[-1][0]                                         \n",
    "                result= self.qa_chain({\"query\": query}, return_only_outputs=True)\n",
    "                #bot_message = textwrap.fill(result['result'], width=500)\n",
    "                bot_message = result['result']\n",
    "                                                         \n",
    "                history[-1][1] = \"\"\n",
    "                for character in bot_message:\n",
    "                    history[-1][1] += character\n",
    "                    time.sleep(0.05)\n",
    "                    yield history\n",
    "                   \n",
    "            def save_chat(history):\n",
    "                with open('{}.json'.format(time.strftime(\"%Y%m%d-%H%M%S\")), 'w', encoding='utf-8') as f:\n",
    "                    json.dump(history, f, ensure_ascii=False, indent=4)\n",
    "                    \n",
    "            #def delete_model():\n",
    "            #    del self.qa_chain\n",
    "        \n",
    "            # Interactive Section\n",
    "            msg.submit(user, [msg, chatbot], [msg, chatbot], queue=False).then(\n",
    "                bot, chatbot, chatbot)\n",
    "            enter_button.click(user, [msg, chatbot], [msg, chatbot], queue=False).then(\n",
    "                bot, chatbot, chatbot)\n",
    "            clear.click(lambda: None, None, chatbot, queue=False)\n",
    "            save_button.click(save_chat, chatbot, None)\n",
    "            #delete_button.click(delete_model, None, queue=False)\n",
    "            \n",
    "            #refresh_button.click(update_dropdown_list, inputs = [], outputs = [docs_dropdown])\n",
    "            \n",
    "            \n",
    "            file_loader.upload(update_doc_and_reload_QA_sys, file_loader).then(\n",
    "                load_state_log, None, state_text)\n",
    "           \n",
    "        demo.queue()\n",
    "        #demo.launch(share=True)\n",
    "        demo.launch()\n",
    "    \n",
    "                                                  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "397f3972-050f-4175-9244-3365286267ca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "QA_UI_CLS = QA_UI()\n",
    "QA_UI_CLS.QA_gradio_UI()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8049e3d9-ff25-4abf-9940-13ef7152b97d",
   "metadata": {},
   "source": [
    "# Create own QA Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48667113-cee9-4fed-b81b-9bc236a04af8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "qa_chain = QA_LangChain_Sys(r'D:\\nu_QA_data\\Nuvoton-MCU-PSG.pdf_index', r'C:\\Users\\USER\\Desktop\\llma\\text-generation-webui\\models\\Llama-2-7b-chat-hf')\n",
    "print(\"===== QA chain build successfully=====\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c42bb1-e483-478d-8929-9069b2653072",
   "metadata": {},
   "source": [
    "# Create a FASIS local save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b58080bb-24f3-4abc-9272-77da37cef37d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def QA_FASIS_VS_save(input_doc_pth, save_path):\n",
    "    # Load doc & split into chunks\n",
    "    print(\"===== Process the document =====\")\n",
    "    loader = UnstructuredFileLoader(input_doc_pth)\n",
    "    documents = loader.load()\n",
    "    text_splitter=CharacterTextSplitter(separator='\\n', chunk_size=1000, chunk_overlap=50)\n",
    "    text_chunks=text_splitter.split_documents(documents)\n",
    "    \n",
    "    # load embedding model\n",
    "    print(\"===== Load the embedding model =====\")\n",
    "    embeddings = HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2',model_kwargs={'device': 'cpu'})\n",
    "    \n",
    "    # Create vectors store\n",
    "    print(\"===== Build FAISS =====\")\n",
    "    vectorstore=FAISS.from_documents(text_chunks, embeddings)\n",
    "    \n",
    "    index_name = input_doc_pth.split('\\\\')[-1] + r'_index'\n",
    "    index_path = os.path.join(save_path, index_name)\n",
    "    print(index_path)\n",
    "    vectorstore.save_local(index_path) \n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6bd37ca-9d9a-4e20-86d8-3693e9d99b05",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "qa_chain = QA_FASIS_VS_save(r'D:\\nu_QA_data\\m460_headers_and_svd.txt', r'D:\\nu_QA_data')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "297ce489-7372-4083-9b18-6c5fce9cb03b",
   "metadata": {},
   "source": [
    "## Download LLM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a39cdd27-29d0-4910-992a-adf1a24d36e0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "login(token = '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')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d9281c-6366-4ac0-9992-95bcd88fef52",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "#tokenizer = AutoTokenizer.from_pretrained(\"togethercomputer/LLaMA-2-7B-32K\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b89661ec-9ea7-4d93-997c-f9a91ba50023",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-hf\", device_map='auto',\n",
    "                       torch_dtype=torch.float16,\n",
    "                       use_auth_token=True,\n",
    "                       #load_in_8bit=True,\n",
    "                        #load_in_4bit=True\n",
    "                       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13fa012b-26fa-43c7-8b63-ca5bf9b7d252",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
