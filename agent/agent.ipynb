{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "255477fb-c252-48a2-b311-2010d1a71d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from pathlib import Path\n",
    "import traceback\n",
    "import json\n",
    "import sys\n",
    "import argparse\n",
    "\n",
    "from langchain.vectorstores import FAISS\n",
    "#from langchain_community.vectorstores import FAISS\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "import json\n",
    "\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "#from langchain.chat_models import ChatOpenAI\n",
    "from langchain_community.chat_models import ChatOpenAI\n",
    "#from langchain_openai import ChatOpenAI\n",
    "\n",
    "#from langchain import PromptTemplate \n",
    "from langchain.prompts import SystemMessagePromptTemplate, HumanMessagePromptTemplate, ChatPromptTemplate, PromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "#from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain_community.embeddings import OpenAIEmbeddings\n",
    "from langchain.callbacks import get_openai_callback\n",
    "import APIKEY\n",
    "os.environ[\"OPENAI_API_KEY\"] = APIKEY.API_KEY_SERVICE_OPENAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4934e0da-da31-427f-98e7-8d036150e377",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import Client\n",
    "\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = f\"Agent_test1\"\n",
    "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = APIKEY.LANGCHAIN_API_KEY  # Update to your API key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b80f2f82-767a-4d57-924f-703e79673d89",
   "metadata": {},
   "source": [
    "## Create the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c72d4a37-0f6a-4100-b324-1d7dff1a5fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#input_doc_pth =r'C:\\Users\\USER\\Desktop\\llma\\LLM_Playing\\pyinstaller_bg\\doc_faiss\\QA\\TRM_M463_M467_pypdf'\n",
    "input_doc_pth = r'D:\\nu_QA_data\\TRM_M463_M467_pypdf_0308'\n",
    "gptmodel = 'gpt-4-1106-preview' #'gpt-3.5-turbo-0125'\n",
    "mmr_num = 8\n",
    "chip_type = 'm460'\n",
    "\n",
    "#system_template = \"\"\"Use the following pieces of context and chat history to answer the question at the end. The context is Nuvoton \"\"\"+chip_type+\"\"\" Series Technical Reference Manual.\n",
    "#If you don't know the answer or the question has nothing to do with technical, don't try to make up an answer.\n",
    "#\"\"\"\n",
    "\n",
    "#system_template = \"\"\"You are a technical engineer to answer chip or math relative question at the end.\n",
    "#You can access two tools, first is searching Nuvoton \"\"\"+chip_type+\"\"\" Series Technical Reference Manual and the second tool is calculator.\n",
    "#Organize the context from searching Technical Reference Manual or calculator to finish the answer.\n",
    "#If you don't know the answer or the question has nothing to do with technical, don't try to make up an answer.\n",
    "#\"\"\"\n",
    "\n",
    "system_template = \"\"\"You are a technical engineer to answer chip or math relative question at the end.\n",
    "You can access three tools, first is searching Nuvoton \"\"\"+chip_type+\"\"\" Series Technical Reference Manual.\n",
    "If you can't get enough infomation, use the second tool, searching tables in Technical Reference Manual.\n",
    "The third tool is calculator.\n",
    "Organize the context from these tools to finish the answer.\n",
    "If you don't know the answer or the question has nothing to do with technical, don't try to make up an answer.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "messages =[(\"system\", system_template),\n",
    "    MessagesPlaceholder(variable_name='chat_history', optional=True),\n",
    "    (\"user\", \"{input}\"),\n",
    "    MessagesPlaceholder(variable_name='agent_scratchpad')]\n",
    "\n",
    "qa_prompt = ChatPromptTemplate.from_messages(messages)\n",
    "\n",
    "llm = ChatOpenAI(temperature=0, model=gptmodel)\n",
    "    \n",
    "# choose your embeddings model\n",
    "embeddings = OpenAIEmbeddings()\n",
    "# FAISS  \n",
    "vectorstore=FAISS.load_local(input_doc_pth, embeddings)\n",
    "retriever_vec=vectorstore.as_retriever(\n",
    "search_type=\"mmr\", # Also test \"similarity\"\n",
    "search_kwargs={\"k\": mmr_num})\n",
    "\n",
    "# Add table retriver\n",
    "input_doc_table_pth = r'D:\\nu_QA_data\\TRM_M463_M467_table_0308'\n",
    "vectorstore_table=FAISS.load_local(input_doc_table_pth, embeddings)\n",
    "retriever_vec_table=vectorstore_table.as_retriever(\n",
    "search_type=\"mmr\", # Also test \"similarity\"\n",
    "search_kwargs={\"k\": 8})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "96ce82c7-6790-4328-bcde-e5df9db69e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools.retriever import create_retriever_tool\n",
    "\n",
    "tool_trm_search = create_retriever_tool(\n",
    "    retriever=retriever_vec,\n",
    "    name = \"search_{}_technical_reference_manual\".format(chip_type),\n",
    "    description = \"Searches and returns detail from the {} technical reference manual.\".format(chip_type),\n",
    ")\n",
    "\n",
    "tool_trm_table_search = create_retriever_tool(\n",
    "    retriever=retriever_vec_table,\n",
    "    name = \"search_{}_technical_reference_manual_tables\".format(chip_type),\n",
    "    description = \"Searches and returns detail from the {} tbales in technical reference manual.\".format(chip_type),\n",
    ")\n",
    "\n",
    "from langchain.tools import BaseTool\n",
    "class EvaluateMathExpression(BaseTool):\n",
    "    name = \"Calculator\"\n",
    "    description = \"Use this tool to evaluate a math expression.\"\n",
    "    def _run(self, expr: str):\n",
    "        return eval(expr)\n",
    "    def _arun(self, query: str):\n",
    "        raise NotImplementedError(\"Async not supported\")\n",
    "\n",
    "tools = [tool_trm_search, tool_trm_table_search, EvaluateMathExpression()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ccae61c2-2176-4e09-b10c-84db00bfd928",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import AgentExecutor, create_openai_tools_agent\n",
    "\n",
    "def create_agent():\n",
    "    agent = create_openai_tools_agent(llm, tools, qa_prompt)\n",
    "    return AgentExecutor(agent=agent, tools=tools)\n",
    "\n",
    "agent_executor = create_agent()\n",
    "chat_history_list = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a73faa-6e17-4368-8a9a-deb9ad2cfc6a",
   "metadata": {},
   "source": [
    "## Start the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a7657069-0972-435f-8dbf-a9b757f7403e",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = agent_executor.invoke({\"input\": \"What is APB and PCLK Maximum Speed ( MHz) of Turbo mode Power Level 0 (PL0)?\",\n",
    "                               \"chat_history\": chat_history_list})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "74f8e308-8c1b-4935-b30d-a5cd90b4f525",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input': 'What is APB and PCLK Maximum Speed ( MHz) of Turbo mode Power Level 0 (PL0)?', 'chat_history': [], 'output': 'The APB and PCLK maximum speed for Turbo mode Power Level 0 (PL0) is not explicitly stated in the provided documents. However, the closest information available is for the Normal mode, where the HCLK maximum speed is 180 MHz and the APB and PCLK maximum speed is 90 MHz at 1.20V. Since Turbo mode typically operates at a higher performance level than Normal mode, it is reasonable to infer that the APB and PCLK maximum speed in Turbo mode PL0 would be equal to or greater than the speed in Normal mode. However, without explicit documentation, I cannot provide a definitive answer for the Turbo mode PL0 speeds.'}\n"
     ]
    }
   ],
   "source": [
    "print(result)\n",
    "chat_history_list.append(HumanMessage(content = result['input']))\n",
    "chat_history_list.append(AIMessage(content = result['output']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "371ae2cf-7554-4b32-9ccd-6a9cb368c5ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = agent_executor.invoke({\"input\": \"What is my name?\",\n",
    "                               \"chat_history\": chat_history_list})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b0c11fa9-2350-4bce-a55f-1bcd1a3c8f4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input': 'What is my name?', 'chat_history': [HumanMessage(content='Hi my name is Gary'), AIMessage(content='Hello Gary! How can I assist you today?')], 'output': 'Your name is Gary. How can I assist you today, Gary?'}\n"
     ]
    }
   ],
   "source": [
    "print(result)\n",
    "chat_history_list.append(HumanMessage(content = result['input']))\n",
    "chat_history_list.append(AIMessage(content = result['output']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1459b123-5876-486c-897a-f74ee20436cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = agent_executor.invoke({\"input\": \"What is 3 * 2 - 1.001 + 3\",\n",
    "                               \"chat_history\": chat_history_list})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d42c008e-4205-4adf-b3fd-3660144b2984",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input': 'What is 3 * 2 - 1.001 + 3', 'chat_history': [HumanMessage(content='Hi my name is Gary'), AIMessage(content='Hello Gary! How can I assist you today?'), HumanMessage(content='What is my name?'), AIMessage(content='Your name is Gary. How can I assist you today, Gary?')], 'output': 'The result of the expression 3 * 2 - 1.001 + 3 is approximately 7.999. If you have any more questions or need further assistance, feel free to ask!'}\n"
     ]
    }
   ],
   "source": [
    "print(result)\n",
    "chat_history_list.append(HumanMessage(content = result['input']))\n",
    "chat_history_list.append(AIMessage(content = result['output']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "007a3da2-2fe4-423e-95af-e7a119dbb32c",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = agent_executor.invoke({\"input\": \"what is the max cpu speed of m460?\",\n",
    "                               \"chat_history\": chat_history_list})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ace8354-7552-4a2e-8ed2-346682bb117d",
   "metadata": {},
   "source": [
    "## New conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1ee2278e-c660-471f-998f-5920cc781eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "51f81e0e-95ae-4edb-ae73-df40662745a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = agent_executor.invoke({\"input\": \"what is the max cpu speed of m460?\",\n",
    "                               \"chat_history\": chat_history_list})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d2718a30-97f3-49c5-b9ea-16facfa69e78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input': 'what is the max cpu speed of m460?', 'chat_history': [], 'output': 'The maximum CPU speed of the Nuvoton M460 series is 200 MHz. This information is derived from the fact that the on-chip PLL (Phase-Locked Loop) can go up to 400 MHz, allowing CPU operation up to the maximum CPU frequency without the need for a high-frequency crystal. However, the specific maximum CPU speed is not directly stated in the provided documents. Typically, the CPU speed is half the PLL frequency due to the internal division, which would suggest a 200 MHz maximum CPU speed for the M460 series.'}\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "534c4ea9-1996-4a42-940d-79d244c11c81",
   "metadata": {},
   "source": [
    "## Config evaluators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8a92d91a-769f-4d3f-8e86-e84b3fd5157e",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_NAME = \"ds_m460_trm_agent\"\n",
    "client = Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3285d6f8-4a63-45b2-b651-b3b09e871404",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import Any, Optional\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain.evaluation import StringEvaluator\n",
    "\n",
    "class GradeEvaluator(StringEvaluator):\n",
    "    \"\"\"An LLM-based relevance evaluator.\"\"\"\n",
    "    def __init__(self):\n",
    "        llm = ChatOpenAI(model=\"gpt-4-0125-preview\", temperature=0)\n",
    "\n",
    "        template = \"\"\"You are a technical professor responsible for checking the correctness of technical issues and \n",
    "grading the students' answers to questions. You are given a question, the student's answer, and the true answer. \n",
    "You are asked to score the student's answer on a scale from 0 to 100.\n",
    "\n",
    "Example Format:\n",
    "QUESTION: question here\n",
    "STUDENT ANSWER: student's answer here\n",
    "TRUE ANSWER: true answer here\n",
    "GRADE: score here between 0 to 100\n",
    "\n",
    "Grade the student answers based ONLY on their factual accuracy and \n",
    "note if the values of the any parameters are correct.\n",
    "Ignore differences in punctuation and phrasing between the student answer and true answer. \n",
    "It is OK if the student answer contains more information than the true answer, \n",
    "as long as it does not contain any conflicting statements. Begin!\n",
    "QUESTION:{input}\n",
    "STUDENT ANSWER:{prediction}\n",
    "TRUE ANSWER:{reference}\n",
    "GRADE:\n",
    "\"\"\"\n",
    "        self.eval_chain = PromptTemplate.from_template(template) | llm\n",
    "\n",
    "    @property\n",
    "    def requires_input(self) -> bool:\n",
    "        return True\n",
    "\n",
    "    @property\n",
    "    def requires_reference(self) -> bool:\n",
    "        return True\n",
    "\n",
    "    @property\n",
    "    def evaluation_name(self) -> str:\n",
    "        return \"scored_relevance\"\n",
    "\n",
    "    def _evaluate_strings(\n",
    "        self,\n",
    "        prediction: str,\n",
    "        input: Optional[str] = None,\n",
    "        reference: Optional[str] = None,\n",
    "        **kwargs: Any\n",
    "    ) -> dict:\n",
    "        evaluator_result = self.eval_chain.invoke(\n",
    "            {\"input\": input, \"prediction\": prediction, \"reference\": reference}, kwargs\n",
    "        )\n",
    "        score = evaluator_result.content\n",
    "        score = re.search(r\"\\d+\", score).group(0)\n",
    "        if score is not None:\n",
    "            score = float(score.strip()) / 100.0\n",
    "        return {\"score\": score}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f9a8552a-3a86-4f37-aa77-1e2f18cb394f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.evaluation import EvaluatorType\n",
    "from langchain.smith import RunEvalConfig\n",
    "from langchain_core.prompts.prompt import PromptTemplate\n",
    "\n",
    "#_PROMPT_TEMPLATE = \"\"\"You are an expert professor specialized in grading students' answers to questions.\n",
    "#You are grading the following question:\n",
    "#{query}\n",
    "#Here is the real answer:\n",
    "#{answer}\n",
    "#You are grading the following predicted answer:\n",
    "#{result}\n",
    "#Respond with CORRECT or INCORRECT:\n",
    "#Grade:\n",
    "#\"\"\"\n",
    "#PROMPT = PromptTemplate(input_variables=['result', 'query', 'answer'], template=_PROMPT_TEMPLATE)\n",
    "\n",
    "evaluation_config = RunEvalConfig(\n",
    "    input_key='input',\n",
    "    prediction_key='output',\n",
    "    evaluators=[\n",
    "        #EvaluatorType.EMBEDDING_DISTANCE,\n",
    "        RunEvalConfig.QA(llm=ChatOpenAI(temperature=0, model='gpt-4-0125-preview')),\n",
    "        RunEvalConfig.LabeledScoreString(\n",
    "            {\n",
    "               \"accuracy\": \"\"\"\n",
    "Score 1: The answer is completely unrelated to the reference.\n",
    "Score 3: The answer has minor relevance but does not align with the reference.\n",
    "Score 5: The answer has moderate relevance but contains inaccuracies.\n",
    "Score 7: The answer aligns with the reference but has minor errors or omissions.\n",
    "Score 10: The answer is completely accurate and aligns perfectly with the reference.\"\"\"\n",
    "            },\n",
    "            normalize_by=10,\n",
    "            llm=ChatOpenAI(temperature=0, model='gpt-4-0125-preview')\n",
    "        ),\n",
    "    ],\n",
    "    # You can add custom StringEvaluator or RunEvaluator objects here as well, which will automatically be\n",
    "    # applied to each prediction. Check out the docs for examples.\n",
    "    custom_evaluators=[GradeEvaluator()]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8462823-d405-40a9-b921-b6b286c8b8e8",
   "metadata": {},
   "source": [
    "## Run the agent and evaluators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f2d7b382-2ea5-4f06-be8a-ae0123d5a93d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for project 'rag2-agent-qa-2' at:\n",
      "https://smith.langchain.com/o/a2f265bc-195d-4980-affd-877d4fb55720/datasets/9990cff8-6b80-4d9c-9143-56b6be7410af/compare?selectedSessions=cc64d4be-e387-42f9-944e-d7b942b1a897\n",
      "\n",
      "View all tests for Dataset ds_m460_trm_agent at:\n",
      "https://smith.langchain.com/o/a2f265bc-195d-4980-affd-877d4fb55720/datasets/9990cff8-6b80-4d9c-9143-56b6be7410af\n",
      "[------------------------------------------------->] 24/24"
     ]
    },
    {
     "data": {
      "text/html": [
       "<h3>Experiment Results:</h3>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feedback.correctness</th>\n",
       "      <th>feedback.score_string:accuracy</th>\n",
       "      <th>feedback.scored_relevance</th>\n",
       "      <th>error</th>\n",
       "      <th>execution_time</th>\n",
       "      <th>run_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>24.000000</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>e6152069-f696-47f5-8878-a97eba29cb10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.804167</td>\n",
       "      <td>0.855417</td>\n",
       "      <td>NaN</td>\n",
       "      <td>21.514409</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.442326</td>\n",
       "      <td>0.256191</td>\n",
       "      <td>0.247790</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.573627</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.699575</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.850000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13.240505</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>18.591372</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>25.078500</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>50.937437</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        feedback.correctness  feedback.score_string:accuracy  \\\n",
       "count              24.000000                       24.000000   \n",
       "unique                   NaN                             NaN   \n",
       "top                      NaN                             NaN   \n",
       "freq                     NaN                             NaN   \n",
       "mean                0.750000                        0.804167   \n",
       "std                 0.442326                        0.256191   \n",
       "min                 0.000000                        0.300000   \n",
       "25%                 0.750000                        0.700000   \n",
       "50%                 1.000000                        1.000000   \n",
       "75%                 1.000000                        1.000000   \n",
       "max                 1.000000                        1.000000   \n",
       "\n",
       "        feedback.scored_relevance error  execution_time  \\\n",
       "count                   24.000000     0       24.000000   \n",
       "unique                        NaN     0             NaN   \n",
       "top                           NaN   NaN             NaN   \n",
       "freq                          NaN   NaN             NaN   \n",
       "mean                     0.855417   NaN       21.514409   \n",
       "std                      0.247790   NaN       10.573627   \n",
       "min                      0.000000   NaN       10.699575   \n",
       "25%                      0.850000   NaN       13.240505   \n",
       "50%                      0.950000   NaN       18.591372   \n",
       "75%                      1.000000   NaN       25.078500   \n",
       "max                      1.000000   NaN       50.937437   \n",
       "\n",
       "                                      run_id  \n",
       "count                                     24  \n",
       "unique                                    24  \n",
       "top     e6152069-f696-47f5-8878-a97eba29cb10  \n",
       "freq                                       1  \n",
       "mean                                     NaN  \n",
       "std                                      NaN  \n",
       "min                                      NaN  \n",
       "25%                                      NaN  \n",
       "50%                                      NaN  \n",
       "75%                                      NaN  \n",
       "max                                      NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import functools\n",
    "\n",
    "from langchain.smith import arun_on_dataset, run_on_dataset\n",
    "\n",
    "chain_results = run_on_dataset(\n",
    "    dataset_name=DATASET_NAME,\n",
    "    llm_or_chain_factory=create_agent(),\n",
    "    evaluation=evaluation_config,\n",
    "    verbose=True,\n",
    "    client=client,\n",
    "    project_name=f\"rag2-agent-qa-2\",\n",
    "    # Project metadata communicates the experiment parameters,\n",
    "    # Useful for reviewing the test results\n",
    "    project_metadata={\n",
    "        \"env\": \"agent.ipynb\",\n",
    "        \"version\": \"0.0.1\",\n",
    "        \"model\": \"gpt-4-1106-preview\",\n",
    "        \"prompt\": \"QA_agent_0308\",\n",
    "        \"input_doc\": \"TRM_M463_M467_pypdf, TRM_M463_M467_table_0308\",\n",
    "        \"langchain_type\": \"create_openai_tools_agent\"\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "898e7392-08fd-47f4-a969-a09d98565ec4",
   "metadata": {},
   "source": [
    "## Copy langsmith dataset and edit examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5d8c0122-0469-4f8d-b804-aba79bb928f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "old_dataset_name = 'ds_m460_trm'\n",
    "examples = client.list_examples(dataset_name=old_dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a5b2781c-ee8c-465d-b340-53516da3d23c",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_inputs = []\n",
    "for x in examples:\n",
    "    data_tuple = (x.inputs['question'], x.outputs['output'])\n",
    "    #print(data_tuple)\n",
    "    example_inputs.append(data_tuple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "437bd103-6c53-456f-abdd-6d37c4b18531",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24\n"
     ]
    }
   ],
   "source": [
    "print(len(example_inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f88a6e07-a4e5-4f40-8a73-e6453e924dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client()\n",
    "dataset_name = \"ds_m460_trm_agent\"\n",
    "\n",
    "# Storing inputs in a dataset lets us\n",
    "# run chains and LLMs over a shared set of examples.\n",
    "dataset = client.create_dataset(\n",
    "    dataset_name=dataset_name, description=\"ds_m460_trm, agent style, only input & output\",\n",
    ")\n",
    "for input_prompt, output_answer in example_inputs:\n",
    "    client.create_example(\n",
    "        inputs={\"input\": input_prompt},\n",
    "        outputs={\"output\": output_answer},\n",
    "        dataset_id=dataset.id,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "747c7739-5cec-4520-9d33-7ab0c4650dcd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
