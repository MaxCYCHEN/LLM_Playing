{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e817c8f7-7a0d-4999-823e-8313247df3b3",
   "metadata": {},
   "source": [
    "## Import LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a2ea5ae-67a8-40fc-b68e-08ff9767a879",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from pathlib import Path\n",
    "import traceback\n",
    "import json\n",
    "import sys\n",
    "import argparse\n",
    "\n",
    "from langchain.vectorstores import FAISS\n",
    "#from langchain_community.vectorstores import FAISS\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "import json\n",
    "\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "#from langchain.chat_models import ChatOpenAI\n",
    "from langchain_community.chat_models import ChatOpenAI\n",
    "#from langchain import PromptTemplate \n",
    "from langchain.prompts import SystemMessagePromptTemplate, HumanMessagePromptTemplate, ChatPromptTemplate\n",
    "#from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain_community.embeddings import OpenAIEmbeddings\n",
    "from langchain.callbacks import get_openai_callback\n",
    "import APIKEY\n",
    "os.environ[\"OPENAI_API_KEY\"] = APIKEY.API_KEY_SERVICE_OPENAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc8df360-1cd0-46f5-bf9b-a90f52251bab",
   "metadata": {},
   "source": [
    "## Import LangSmith"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e5531185-1384-4450-87d2-4ccf2ea61b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c9e373a2-ca51-4440-ba11-d4ec2e624ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "#os.environ[\"LANGCHAIN_PROJECT\"] = f\"Tracing Walkthrough - Test2\"\n",
    "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = APIKEY.LANGCHAIN_API_KEY  # Update to your API key\n",
    "\n",
    "DATASET_NAME = \"ds_m460_trm\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "91bf7aaa-4281-43d9-a457-3715645cc03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client()\n",
    "datasets = client.list_datasets(dataset_name=DATASET_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "38c34025-2120-4580-9704-de807d3e57d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = client.list_examples(dataset_name=DATASET_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "34eaf0b8-cbfa-4d23-9df8-6129c54877e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset_id=UUID('a117ee3a-5545-4f7f-bb72-95f756df6116') inputs={'question': 'Can M460 TRNG generate 1000 random bits per second?', 'chat_history': ''} outputs={'output': 'No, the True Random Number Generator (TRNG) in the M460 series is capable of generating 800 random bits per second, as stated in the technical reference manual.'} id=UUID('09321f02-5cb7-4054-aa87-47329624483b') created_at=datetime.datetime(2024, 2, 23, 7, 7, 56, 147914, tzinfo=datetime.timezone.utc) modified_at=datetime.datetime(2024, 2, 27, 8, 32, 45, 224832, tzinfo=datetime.timezone.utc) runs=[] source_run_id=None\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, 1):\n",
    "    print(next(examples))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b55b3b9e-3508-4764-bb6a-cb4af83974e5",
   "metadata": {},
   "source": [
    "## Create the chain be tested"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e05d5fee-c191-4527-9906-33756f7dd293",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_doc_pth =r'C:\\Users\\USER\\Desktop\\llma\\LLM_Playing\\pyinstaller_bg\\doc_faiss\\QA\\TRM_M463_M467_pypdf'\n",
    "\n",
    "gptmodel = 'gpt-4-0125-preview' #'gpt-4-0125-preview'\n",
    "\n",
    "mmr_num = 5\n",
    "\n",
    "chip_type = 'm460'\n",
    "system_template = \"\"\"Use the following pieces of context and chat history to answer the question at the end. The context is Nuvoton \"\"\"+chip_type+\"\"\" Series Technical Reference Manual.\n",
    "If you don't know the answer or the question has nothing to do with technical, don't try to make up an answer.\n",
    "----------------\n",
    "{context}\n",
    "{chat_history}\"\"\"\n",
    "messages = [\n",
    "        SystemMessagePromptTemplate.from_template(system_template),\n",
    "        HumanMessagePromptTemplate.from_template(\"{question}\")\n",
    "        ]\n",
    "qa_prompt = ChatPromptTemplate.from_messages(messages)\n",
    "\n",
    "\n",
    "def QA_LangChain_RQA_chain():\n",
    "    llm = ChatOpenAI(temperature=0, model=gptmodel)\n",
    "    \n",
    "    # load embedding model\n",
    "    print(\"===== Load the embedding model =====\", flush=True)\n",
    "    \n",
    "    # choose your embeddings model\n",
    "    embeddings = OpenAIEmbeddings()\n",
    "\n",
    "    # FAISS  \n",
    "    vectorstore=FAISS.load_local(input_doc_pth, embeddings)\n",
    "    retriever_vec=vectorstore.as_retriever(\n",
    "    search_type=\"mmr\", # Also test \"similarity\"\n",
    "    search_kwargs={\"k\": mmr_num})\n",
    "    \n",
    "    print(\"===== Create a ConversationalRetrievalChain chain =====\", flush=True)\n",
    "    # Normal memory\n",
    "    memory = ConversationBufferMemory(memory_key=\"chat_history\", input_key='question', output_key='answer', return_messages=True)\n",
    "    \n",
    "    chain = ConversationalRetrievalChain.from_llm(llm=llm, retriever=retriever_vec, memory=memory,\n",
    "                                                      return_source_documents=True, \n",
    "                                                      combine_docs_chain_kwargs={\"prompt\": qa_prompt}\n",
    "                                                     )\n",
    "    \n",
    "    return chain\n",
    "\n",
    "#test_chain = QA_LangChain_RQA_chain()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19cb0ba2-ff84-48ae-8a41-957f95a9b1e9",
   "metadata": {},
   "source": [
    "## Config evaluators"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae525857-37c9-4e31-b02e-f2b2b647dae9",
   "metadata": {},
   "source": [
    "#### - Custom evaluator that logs a heuristic evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dd1fe7b4-b3da-4f1f-9ef1-e26ff0fe7a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith.evaluation import EvaluationResult, run_evaluator\n",
    "from langsmith.schemas import Example, Run\n",
    "\n",
    "\n",
    "@run_evaluator\n",
    "def check_not_idk(run: Run, example: Example):\n",
    "    \"\"\"Illustration of a custom evaluator.\"\"\"\n",
    "    agent_response = run.outputs[\"answer\"]\n",
    "    if \"don't know\" in agent_response or \"not sure\" in agent_response:\n",
    "        score = 0\n",
    "    else:\n",
    "        score = 1\n",
    "    # You can access the dataset labels in example.outputs[key]\n",
    "    # You can also access the model inputs in run.inputs[key]\n",
    "    return EvaluationResult(\n",
    "        key=\"not_uncertain\",\n",
    "        score=score,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9fe0a92-b27d-418c-afe0-672a57154947",
   "metadata": {},
   "source": [
    "#### - Custom LangChain string evaluators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "863f1946-7c35-4507-b993-72ca390ed011",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import Any, Optional\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain.evaluation import StringEvaluator\n",
    "\n",
    "class GradeEvaluator(StringEvaluator):\n",
    "    \"\"\"An LLM-based relevance evaluator.\"\"\"\n",
    "    def __init__(self):\n",
    "        llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\", temperature=0)\n",
    "\n",
    "        template = \"\"\"You are a technical professor responsible for checking the correctness of technical issues and \n",
    "grading the students' answers to questions. You are given a question, the student's answer, and the true answer. \n",
    "You are asked to score the student's answer on a scale from 0 to 100.\n",
    "\n",
    "Example Format:\n",
    "QUESTION: question here\n",
    "STUDENT ANSWER: student's answer here\n",
    "TRUE ANSWER: true answer here\n",
    "GRADE: score here between 0 to 100\n",
    "\n",
    "Grade the student answers based ONLY on their factual accuracy and \n",
    "note if the values of the any parameters are correct.\n",
    "Ignore differences in punctuation and phrasing between the student answer and true answer. \n",
    "It is OK if the student answer contains more information than the true answer, \n",
    "as long as it does not contain any conflicting statements. Begin!\n",
    "QUESTION:{input}\n",
    "STUDENT ANSWER:{prediction}\n",
    "TRUE ANSWER:{reference}\n",
    "GRADE:\n",
    "\"\"\"\n",
    "        self.eval_chain = PromptTemplate.from_template(template) | llm\n",
    "\n",
    "    @property\n",
    "    def requires_input(self) -> bool:\n",
    "        return True\n",
    "\n",
    "    @property\n",
    "    def requires_reference(self) -> bool:\n",
    "        return True\n",
    "\n",
    "    @property\n",
    "    def evaluation_name(self) -> str:\n",
    "        return \"scored_relevance\"\n",
    "\n",
    "    def _evaluate_strings(\n",
    "        self,\n",
    "        prediction: str,\n",
    "        input: Optional[str] = None,\n",
    "        reference: Optional[str] = None,\n",
    "        **kwargs: Any\n",
    "    ) -> dict:\n",
    "        evaluator_result = self.eval_chain.invoke(\n",
    "            {\"input\": input, \"prediction\": prediction, \"reference\": reference}, kwargs\n",
    "        )\n",
    "        score = evaluator_result.content\n",
    "        score = re.search(r\"\\d+\", score).group(0)\n",
    "        if score is not None:\n",
    "            score = float(score.strip()) / 100.0\n",
    "        return {\"score\": score}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c61dba4-1cdc-447f-bc13-5c48c17e0580",
   "metadata": {},
   "source": [
    "#### - Combine the custom & build-in evaluators in RunEvalConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "880417d0-4c1f-4ab1-a57f-5443c4c6a685",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\miniforge3\\envs\\LC_gradio\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.chat_models.openai.ChatOpenAI` was deprecated in langchain-community 0.0.10 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import ChatOpenAI`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "from langchain.evaluation import EvaluatorType\n",
    "from langchain.smith import RunEvalConfig\n",
    "from langchain_core.prompts.prompt import PromptTemplate\n",
    "\n",
    "#_PROMPT_TEMPLATE = \"\"\"You are an expert professor specialized in grading students' answers to questions.\n",
    "#You are grading the following question:\n",
    "#{query}\n",
    "#Here is the real answer:\n",
    "#{answer}\n",
    "#You are grading the following predicted answer:\n",
    "#{result}\n",
    "#Respond with CORRECT or INCORRECT:\n",
    "#Grade:\n",
    "#\"\"\"\n",
    "#PROMPT = PromptTemplate(input_variables=['result', 'query', 'answer'], template=_PROMPT_TEMPLATE)\n",
    "\n",
    "evaluation_config = RunEvalConfig(\n",
    "    input_key='question',\n",
    "    prediction_key='answer',\n",
    "    evaluators=[\n",
    "        #EvaluatorType.EMBEDDING_DISTANCE,\n",
    "        RunEvalConfig.QA(llm=ChatOpenAI(temperature=0, model='gpt-3.5-turbo-0125')),\n",
    "        RunEvalConfig.LabeledScoreString(\n",
    "            {\n",
    "               \"accuracy\": \"\"\"\n",
    "Score 1: The answer is completely unrelated to the reference.\n",
    "Score 3: The answer has minor relevance but does not align with the reference.\n",
    "Score 5: The answer has moderate relevance but contains inaccuracies.\n",
    "Score 7: The answer aligns with the reference but has minor errors or omissions.\n",
    "Score 10: The answer is completely accurate and aligns perfectly with the reference.\"\"\"\n",
    "            },\n",
    "            normalize_by=10,\n",
    "            llm=ChatOpenAI(temperature=0, model='gpt-4-0125-preview')\n",
    "        ),\n",
    "    ],\n",
    "    # You can add custom StringEvaluator or RunEvaluator objects here as well, which will automatically be\n",
    "    # applied to each prediction. Check out the docs for examples.\n",
    "    custom_evaluators=[GradeEvaluator()]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "855458ac-14fc-4656-84b3-a6190cb5055e",
   "metadata": {},
   "source": [
    "## Run the agent and evaluators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b0d32ed0-61ee-4ef6-b35e-014c1d1401e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Load the embedding model =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\miniforge3\\envs\\LC_gradio\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.embeddings.openai.OpenAIEmbeddings` was deprecated in langchain-community 0.1.0 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAIEmbeddings`.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Create a ConversationalRetrievalChain chain =====\n",
      "View the evaluation results for project 'rag-chain-qa-1' at:\n",
      "https://smith.langchain.com/o/a2f265bc-195d-4980-affd-877d4fb55720/datasets/a117ee3a-5545-4f7f-bb72-95f756df6116/compare?selectedSessions=be988627-6c73-485e-88ea-f5ef04ce35d2\n",
      "\n",
      "View all tests for Dataset ds_m460_trm at:\n",
      "https://smith.langchain.com/o/a2f265bc-195d-4980-affd-877d4fb55720/datasets/a117ee3a-5545-4f7f-bb72-95f756df6116\n",
      "===== Load the embedding model =====\n",
      "===== Create a ConversationalRetrievalChain chain =====\n",
      "===== Load the embedding model =====\n",
      "===== Create a ConversationalRetrievalChain chain =====\n",
      "===== Load the embedding model =====\n",
      "===== Create a ConversationalRetrievalChain chain =====\n",
      "[>                                                 ] 0/25===== Load the embedding model ========== Load the embedding model =====\n",
      "\n",
      "===== Load the embedding model =====\n",
      "===== Load the embedding model =====\n",
      "===== Load the embedding model =====\n",
      "===== Create a ConversationalRetrievalChain chain =====\n",
      "===== Create a ConversationalRetrievalChain chain =====\n",
      "===== Create a ConversationalRetrievalChain chain =====\n",
      "===== Create a ConversationalRetrievalChain chain =====\n",
      "===== Create a ConversationalRetrievalChain chain =====\n",
      "[->                                                ] 1/25===== Load the embedding model =====\n",
      "===== Create a ConversationalRetrievalChain chain =====\n",
      "[--->                                              ] 2/25===== Load the embedding model =====\n",
      "===== Create a ConversationalRetrievalChain chain =====\n",
      "[------->                                          ] 4/25===== Load the embedding model =====\n",
      "===== Load the embedding model =====\n",
      "===== Create a ConversationalRetrievalChain chain =====\n",
      "===== Create a ConversationalRetrievalChain chain =====\n",
      "[--------->                                        ] 5/25===== Load the embedding model =====\n",
      "===== Create a ConversationalRetrievalChain chain =====\n",
      "[----------->                                      ] 6/25===== Load the embedding model =====\n",
      "===== Create a ConversationalRetrievalChain chain =====\n",
      "[------------->                                    ] 7/25===== Load the embedding model =====\n",
      "===== Create a ConversationalRetrievalChain chain =====\n",
      "[--------------->                                  ] 8/25===== Load the embedding model =====\n",
      "===== Create a ConversationalRetrievalChain chain =====\n",
      "[----------------->                                ] 9/25===== Load the embedding model =====\n",
      "===== Create a ConversationalRetrievalChain chain =====\n",
      "[------------------->                              ] 10/25===== Load the embedding model =====\n",
      "===== Create a ConversationalRetrievalChain chain =====\n",
      "[--------------------->                            ] 11/25===== Load the embedding model =====\n",
      "===== Create a ConversationalRetrievalChain chain =====\n",
      "[----------------------->                          ] 12/25===== Load the embedding model =====\n",
      "===== Create a ConversationalRetrievalChain chain =====\n",
      "[------------------------->                        ] 13/25===== Load the embedding model =====\n",
      "===== Create a ConversationalRetrievalChain chain =====\n",
      "[--------------------------->                      ] 14/25===== Load the embedding model =====\n",
      "===== Create a ConversationalRetrievalChain chain =====\n",
      "[----------------------------->                    ] 15/25===== Load the embedding model =====\n",
      "===== Create a ConversationalRetrievalChain chain =====\n",
      "[------------------------------->                  ] 16/25===== Load the embedding model =====\n",
      "===== Create a ConversationalRetrievalChain chain =====\n",
      "[--------------------------------->                ] 17/25===== Load the embedding model =====\n",
      "===== Create a ConversationalRetrievalChain chain =====\n",
      "[----------------------------------->              ] 18/25===== Load the embedding model =====\n",
      "===== Create a ConversationalRetrievalChain chain =====\n",
      "[------------------------------------->            ] 19/25===== Load the embedding model =====\n",
      "===== Create a ConversationalRetrievalChain chain =====\n",
      "[--------------------------------------->          ] 20/25===== Load the embedding model =====\n",
      "===== Create a ConversationalRetrievalChain chain =====\n",
      "[------------------------------------------------->] 25/25"
     ]
    },
    {
     "data": {
      "text/html": [
       "<h3>Experiment Results:</h3>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feedback.correctness</th>\n",
       "      <th>feedback.score_string:accuracy</th>\n",
       "      <th>feedback.scored_relevance</th>\n",
       "      <th>error</th>\n",
       "      <th>execution_time</th>\n",
       "      <th>run_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>24.000000</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>f969b26c-7653-466e-97a8-d8a4c37c9ecd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.791667</td>\n",
       "      <td>0.641667</td>\n",
       "      <td>0.772917</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16.201695</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.414851</td>\n",
       "      <td>0.333514</td>\n",
       "      <td>0.232182</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11.824602</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.984836</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.865446</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.850000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.620600</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>24.473121</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>40.010856</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        feedback.correctness  feedback.score_string:accuracy  \\\n",
       "count              24.000000                       24.000000   \n",
       "unique                   NaN                             NaN   \n",
       "top                      NaN                             NaN   \n",
       "freq                     NaN                             NaN   \n",
       "mean                0.791667                        0.641667   \n",
       "std                 0.414851                        0.333514   \n",
       "min                 0.000000                        0.100000   \n",
       "25%                 1.000000                        0.300000   \n",
       "50%                 1.000000                        0.700000   \n",
       "75%                 1.000000                        1.000000   \n",
       "max                 1.000000                        1.000000   \n",
       "\n",
       "        feedback.scored_relevance error  execution_time  \\\n",
       "count                   24.000000     0       25.000000   \n",
       "unique                        NaN     0             NaN   \n",
       "top                           NaN   NaN             NaN   \n",
       "freq                          NaN   NaN             NaN   \n",
       "mean                     0.772917   NaN       16.201695   \n",
       "std                      0.232182   NaN       11.824602   \n",
       "min                      0.000000   NaN        3.984836   \n",
       "25%                      0.750000   NaN        5.865446   \n",
       "50%                      0.850000   NaN       10.620600   \n",
       "75%                      0.900000   NaN       24.473121   \n",
       "max                      1.000000   NaN       40.010856   \n",
       "\n",
       "                                      run_id  \n",
       "count                                     25  \n",
       "unique                                    25  \n",
       "top     f969b26c-7653-466e-97a8-d8a4c37c9ecd  \n",
       "freq                                       1  \n",
       "mean                                     NaN  \n",
       "std                                      NaN  \n",
       "min                                      NaN  \n",
       "25%                                      NaN  \n",
       "50%                                      NaN  \n",
       "75%                                      NaN  \n",
       "max                                      NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import functools\n",
    "\n",
    "from langchain.smith import arun_on_dataset, run_on_dataset\n",
    "\n",
    "chain_results = run_on_dataset(\n",
    "    dataset_name=DATASET_NAME,\n",
    "    llm_or_chain_factory=QA_LangChain_RQA_chain,\n",
    "    evaluation=evaluation_config,\n",
    "    verbose=True,\n",
    "    client=client,\n",
    "    project_name=f\"rag-chain-qa-1\",\n",
    "    # Project metadata communicates the experiment parameters,\n",
    "    # Useful for reviewing the test results\n",
    "    project_metadata={\n",
    "        \"env\": \"evl_chain.ipynb\",\n",
    "        \"model\": \"gpt-4-0125-preview\",\n",
    "        \"prompt\": \"QA_0226\",\n",
    "        \"input_doc\": \"TRM_M463_M467_pypdf\",\n",
    "        \"langchain_type\": \"ConversationalRetrievalChain\"\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d028395c-e889-47a2-b460-43106c49915a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
